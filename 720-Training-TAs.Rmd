# Training Teaching Assistants {#tatraining720}

In our program, GTAs direct the undergraduate lab sections and grade lab reports. Most of our incoming GTAs have not graded student work before, and bring a variety of past experiences, biases, and misonceptions with them when they join our program. Many have limited scientific reading and writing experience so may have less confidence in their ability to judge the work of students who are close to their own age. To compensate, less experienced GTAs often focus on obvious mechanical and formatting errors, citation punctuation, and similar items that can be corrected by copy-editing. 

We have implemented 3 training activities that help new GTAs internalize and start using our bins-oriented strategy [explained elsewhere](#commenting710). 

* A general orientation to our approach. This comes in two parts: during the first day of orientation for new GTAs, and during the lab prep meeting ~ 2 weeks before GTAs begin teaching writing for the first time.
* Marking up previously graded reports. Before grading their first time, GTAs are given a set of training reports and asked to grade them using our bins-based scoring. After grading, GTAs discuss their scores and comments with experienced graders.
* Round-robin scoring. Each GTA selects 4 reports from their first set of the semester, score them, then passes the 4 reports to another TA who also scores them. Scores and discrepancies are discussed in the next lab prep meeting.

Seasoned GTAs can bring confounding preconceptions from their previous schools, or incorporate writing conventions that are specific to their research field. Their grading performance and priorities can drift over time as well. To limit these problems, we ask experienced GTAs to help train the incoming novice GTAs each fall semester. As they explain our grading strategy to their peers, most will self-correct.

We also run correlation analysis comparing students' lecture and lab grades at the end of each semester. Historically, student lecture and lab grades have a correlation > 0.85, and correlation slopes do not vary much between GTAs who are teaching in the same course. Signs that a particular GTA may not be grading according to the criteria include a difference of >3% in median report scores relative to other GTAs in the same coure, a lecture/lab grade correlation < 0.8, or a correlation slope that differs significantly from that of other GTAs in the course. 

Finally, we try to spot check the types of comments GTAs make on reports at least once each year. We collect 4-6 randomly selected reports for each GTA, and use a standardized codebook<sup>1</sup> to classify the types and numbers of comments they are attaching to reports according to:

1. __Subject__. What does each comment focus on (basic criteria, writing flaws, logic, etc.)? 
2. __Structure__. How is the comment worded? Is the comment a simple pointer or informative? Is there general or specific information contained in comment? Is it directive only, or does the comment foster broader thinking?
3. __Agency__. Where is the locus of control in the comment? Is the instructor the primary source of knowledge, or does the student retain agency and choice? Does the comment provide explicit directions, or ask the student to reflect on their writing issues and discover their own answers? 

When a GTA's grading deviates from expectations or past performance, we meet with them to discuss their grading strategy and find ways to make corrections going forward.

<br/>
<hr/>

<sup>1</sup> This is excerpted from a larger classification schema we developed for an [automated comment classifier project](https://adanieljohnson.github.io/default_website/codebook.html). 
